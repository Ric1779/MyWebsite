---
title: "Gaussian Splatting ðŸ””"
date: 2024-05-01T23:15:00+09:00
slug: GS    
category: GS
summary:
description:
cover:
  image: "covers/corgi_3.jpg"
  alt:
  caption:
  relative: true
showtoc: true
draft: false
---

## Introduction
---
#### Overview of Gaussian Splatting

In this project, we explore Gaussian splatting, a rendering technique predominantly utilized in computer graphics and visualization. Gaussian splatting differs from traditional rendering methods by using Gaussian distributions to project geometric primitives. This approach enables the creation of images with high detail and realism, which is particularly beneficial in fields like medical imaging and scientific visualization where detail and accuracy are crucial.

Gaussian splatting maps data points, which could represent various entities from image pixels to scientific data, onto a visualization space using Gaussian functions. This technique results in a smoother and more continuous representation of data, as opposed to the pixelated or discretized outputs commonly produced by traditional methods.

#### Purpose and Benefits of Using a Synthetic Dataset

The efficacy of a machine learning model heavily relies on the quality of the training dataset. However, acquiring large amounts of high-quality, real-world data can be challenging, costly, and time-consuming. Synthetic datasets address these challenges by offering:

1. **Control Over Data Attributes:** They allow for the generation of data with specific, controlled attributes and conditions, which can be difficult to obtain from real-world sources.
2. **Enhanced Model Robustness:** Training models with synthetic data under varied, controlled scenarios can improve their robustness and ability to generalize.
3. **Cost-Effectiveness:** Synthetic datasets reduce the costs associated with the collection of real-world data, especially in domains where acquiring such data is prohibitively expensive.
4. **Privacy Compliance:** They help in avoiding privacy concerns associated with the use of real-world data, particularly in sensitive sectors such as healthcare.

For this project, a synthetic dataset was created using a 3D model of a corgi in Blender. This method not only served to illustrate the process of generating synthetic data but also highlighted the practical application of Gaussian splatting in a controlled, reproducible manner.

The project involved setting up the required environment, designing the 3D model, generating the synthetic dataset, and subsequently, training and evaluating a Gaussian splatting model using PyTorch. Each phase was crafted to provide detailed insights into both the technical processes and the theoretical foundations that underscore the utility of Gaussian splatting in advanced visualization and machine learning applications.

The completion of this project provided a comprehensive understanding of Gaussian splatting, demonstrated the advantages of synthetic datasets, and offered practical experience in implementing these techniques using advanced tools and technologies.

## Generating the Synthetic Dataset
---
#### Setting Up the Scene in Blender

For the generation of a synthetic dataset, the initial step involved setting up a detailed 3D scene using Blender. We started by clearing any existing objects and lights from the scene to ensure a controlled environment. This was done using a Python script that automates the process of clearing the scene, thus preventing interference from any prior configurations.

#### Importing the 3D Model

The corgi model, central to our dataset, was imported into Blender. The script supported various file formats such as OBJ, STL, FBX, DAE, PLY, and GLTF, offering flexibility depending on the source file of the 3D model. Once imported, the model was positioned and scaled appropriately within the scene to ensure that it fit well within the camera's view at all times during the rendering process.

{{< rawhtml>}}
<p align="center">
  <img src="../images/GS/00017-alpha.jpg" alt="Image description" class="img-fluid" style="max-width: 100%; height: auto; border-radius: 10px; width: 100%"/>
</p>
<p align="center">
  <em>Figure 1: Dataset Sample</em>
</p>
{{< /rawhtml>}}

#### Camera and Lighting Configuration

To mimic real-world conditions and capture the corgi model from multiple perspectives, we programmatically configured multiple cameras around the scene. The script allowed for the setting of camera positions and orientations to cover a wide range of angles, crucial for training the Gaussian splatting model with diverse visual data.

Lighting plays a crucial role in defining the appearance and depth of the model in rendered images. Using the script, we set up uniform lighting conditions that were consistently reproduced across all renderings. This uniformity is vital for minimizing variables in the training data other than the modelâ€™s orientation and camera angle.

#### Automating Image Generation

The core of the data generation process was the automated rendering of the corgi model under various conditions. The script generated a specified number of images, each from different camera angles and with consistent lighting settings. This automation was crucial for producing a large and varied dataset efficiently.

Each rendered image was saved along with its metadata, which included the camera coordinates and specific lighting settings. This metadata is essential for the Gaussian splatting model training, as it helps in correlating the input images with the camera and lighting conditions under which they were produced.

```json
{"backend": "BLENDER_EEVEE", "light_mode": "uniform", "fast_mode": false, "format_version": 6, 
"channels": ["R", "G", "B", "A","D"], "scale": 0.5, "images": [{"intrinsic": [3x3], "pose": [4x4], 
"rgb": "00000_rgb.png","depth": "00000_depth.png", "alpha": "00000_alpha.png", "max_depth": 5.0, 
"HW": [512, 512]},
```

#### Ensuring Consistency and Quality

To ensure the quality and consistency of the dataset, the script included parameters to control the rendering process such as the choice of rendering engine (e.g., Blender's Eevee or Cycles) and the resolution of the output images. Options were also available for fast rendering modes that decrease the time required to generate the dataset, albeit at a potential cost to image quality, which can be useful for preliminary testing of the model.

In summary, generating the synthetic dataset involved meticulous setup of the 3D scene, careful configuration of camera and lighting, and efficient automation of the image rendering process. This dataset serves as a critical resource for training the Gaussian splatting model, providing it with a wide range of images that capture the detailed nuances of the corgi model under varied simulated conditions.

## Transforming Datasets into Rich Point Clouds
---
The transformation of dataset into point clouds is a critical step for applications ranging from augmented reality to autonomous navigation. Traditional methods often focus merely on the geometric aspects of point clouds. However, in the implemented approach using the `GaussModel` and `point_utils` introduces an advanced method that enhances point clouds with detailed physical and visual attributes. Here's a comprehensive guide on this sophisticated transformation is achieved.

#### Step 1: Generating Rays from Camera Data

The journey begins with the `get_rays_single_image` function from `point_utils.py`, which calculates the origin and direction of rays for each pixel in the synthetic images. This function uses the camera's intrinsic parameters and its position and orientation in the world (extrinsic parameters) to map each pixel onto a ray in 3D space.

**Key Process:**
- **Meshgrid Creation:** For an image of width `W` and height `H`, we generate a grid representing the pixel coordinates.
- **Ray Calculation:** Utilizing the intrinsic and extrinsic matrices, we compute the direction of rays from the camera center through each pixel.

#### Step 2: Converting Depth Maps to Point Clouds

Next, the `get_point_clouds` function converts these rays and their corresponding depth values from the depth maps into 3D points. This function takes into account the alpha values to filter out the occluded or irrelevant points, integrating only visible points into the final point cloud.

```python
depth = torch.from_numpy(imageio.imread('').astype(np.float32) / 255.0 * max_depth)
pts = rays_o + rays_d * depth
```
**Enhancements:**
- **Visibility Handling:** Points are selected based on their alpha values, ensuring transparency and occlusion are properly managed.
- **Color Integration:** If RGB data is provided, it is concatenated with the alpha data to include color in the point cloud, enriching the visualization aspect.

#### Step 3: Modeling Points as Gaussians

The unique contribution of Gaussian Splatting lies in the `GaussModel`, which treats each point not just as a coordinate in space but as a Gaussian distribution. This model adds several layers of complexity and utility by incorporating additional attributes:

- **Scaling and Rotation:** Each point's spread and orientation are controlled, allowing the point cloud to more accurately represent surface details and textures.
- **Opacity:** Transparency is managed at a point level, enabling the rendering of complex visual phenomena like shadows and light diffusion.
- **Spherical Harmonics for Color Mapping:** The most innovative feature is using spherical harmonics coefficients to map color onto the points. This method allows for a detailed representation of surface colors and textures under varying lighting conditions.

By treating each point in our dataset as a Gaussian with comprehensive physical and visual properties, we significantly enhance the realism and applicability of synthetic point clouds. Gaussian Splatting revolves around refining these parameters to more accurately depict the 3D environment, employing techniques akin to Gradient Descent. This approach not only improves the aesthetic quality of the visualizations but also increases the functional accuracy for simulations in virtual environments. Whether for advancing graphic rendering techniques or enhancing the perception systems of robots.

## Camera Class
---
In this section, we delve into the practical application of camera class for Gaussian Rendering. The camera class implemented in `camera_utils.py` plays a pivotal role in facilitating this process during the training phase of Gaussian Splatting.

### Camera Class Setup

The `Camera` class, as defined in the utility, encapsulates all necessary camera parameters required for rendering. This includes the intrinsic matrix, camera-to-world transformation, field of view, and projection matrix. Hereâ€™s a breakdown of its setup:

- **Intrinsic and Extrinsic Parameters:** The cameraâ€™s intrinsic matrix and camera-to-world (c2w) matrix are crucial for defining the lens characteristics and the cameraâ€™s position and orientation in the world, respectively.

- **Field of View Calculation:** Using the intrinsic parameters, the field of view (FoV) for both x and y axes is calculated. This FoV is essential for determining how wide the camera can see, which is directly used in rendering calculations.

- **Projection Matrix Construction:** The projection matrix is constructed based on the near and far clipping planes along with the FoV. This matrix is used to transform 3D camera coordinates to 2D screen coordinates, critical for any rendering process.

### Training with Gaussian Rendering

During the training phase, Gaussian Rendering with EWA Splatting is employed to enhance the quality of the rendered images from point clouds or volumetric data. This rendering technique uses the camera parameters to project 3D points onto a 2D plane accurately:

- **Gaussian Filtering:** EWA Splatting involves applying a Gaussian filter to the image space projection of data points. This filtering helps in reducing aliasing artifacts and improves image quality, making it highly suitable for training deep learning models that require high-quality input data, explained more elaborately in further sections.

- **Handling Edge Cases:** The `Camera` class handles scenarios where the projection might lead to distortions or where points fall outside the cameraâ€™s view. This robust handling ensures that the rendered outputs are consistent and usable for further processing or analysis.

The integration of the `Camera` class in the training pipeline allows for precise control over the rendering process, which is crucial for the accuracy of the Gaussian Rendering using EWA Splatting. By adjusting camera parameters and employing sophisticated rendering techniques, developers can significantly enhance the visual quality of the 3D data used in training models, leading to better performance and more realistic simulations.

## Training Phase
---
In the training phase, precise management of computational resources and efficient utilization of data are paramount. The Python files, `train.py` and `trainer.py`, illustrate a sophisticated training infrastructure designed to optimize these processes. This section will detail the structure and techniques employed during the training phase to achieve high-quality 3D rendering.

### Training Infrastructure

The training process is managed through two primary classes: `Trainer` and `GSSTrainer`. `Trainer` provides a general framework, while `GSSTrainer` extends it to incorporate specific methods for Gaussian Rendering.

#### The `Trainer` Class

- **General Framework:** This class sets up a generalized training loop that handles the basic operations of any training process, such as batch processing, loss calculation, and gradient updates.
- **Acceleration and Scalability:** It is designed to be compatible with hardware accelerators, which are crucial for handling large datasets and complex rendering computations efficiently.

#### The `GSSTrainer` Class

- **Gaussian Rendering Integration:** This class integrates the Gaussian Renderer, a critical component for training models to perform high-quality 3D rendering using Gaussian Rendering with EWA Splatting.
- **Dynamic Loss Calculation:** It utilizes a combination of L1 loss, Structural Similarity Index Measure (SSIM), and depth loss, dynamically weighting these during training to fine-tune the rendering output. The use of SSIM helps in maintaining the perceptual quality of the renderings.
- **Performance Profiling:** Optionally, the training can be profiled using PyTorch's profiler to identify bottlenecks and optimize the training process.

### Loss Function and Metrics

The training phase leverages a composite loss function:
- **L1 Loss** ensures the rendered image's pixel-wise accuracy.
- **SSIM Loss** optimizes for structural similarities between the rendered and target images, emphasizing texture and depth integrity.
- **Depth Loss** is applied conditionally, focusing on the accuracy of depth predictions, which is crucial for realistic rendering.

These loss components are combined based on their respective weights, which can be dynamically adjusted to prioritize different aspects of the image quality during training.

### Optimization and Logging

- **Optimization Steps:** The training loop includes steps for backpropagation, optimization, and logging. It employs gradient accumulation to stabilize the training with high learning rates or large mini-batches.
- **Detailed Logging:** Throughout the training process, detailed logs are generated, capturing loss metrics and other performance indicators, which are essential for monitoring the training progress and making adjustments as needed.

The training setup described here is robust, allowing for high configurability and adaptability. It supports extensive customizations to cater to specific needs of 3D rendering tasks, ensuring that the models trained are not only efficient but also capable of producing state-of-the-art results in Gaussian Rendering.

## Rendering
---
### Calculating the 2D and 3D Covariances

In Gaussian Splatting, understanding and computing 2D and 3D covariances is essential. These covariances represent the spread and orientation of data points in both two-dimensional image space and three-dimensional world space, respectively.

#### 3D Covariance Calculation

The 3D covariance is crucial for capturing the spatial distribution and orientation of the Gaussian kernels in the 3D space around each point in the point cloud. Hereâ€™s how the 3D covariance is constructed:

- **Scaling and Rotation:** Each point in the point cloud has associated scale and rotation parameters. The scaling parameters adjust the size of the Gaussian kernel along each axis, whereas the rotation parameters adjust its orientation.
- **Construction of Transformation Matrices:** A scaling matrix and a rotation matrix are constructed based on these parameters. The combined effect of these matrices gives a transformation matrix that, when applied to the identity matrix, yields the 3D covariance matrix.
- **Final 3D Covariance:** The transformation matrix is then used to build the 3D covariance matrix by applying the transformation to the identity covariance (a simple sphere in 3D), stretching and rotating it according to the scale and orientation of each point.

#### 2D Covariance Calculation and EWA Splatting

The calculation of 2D covariances involves projecting these 3D covariances onto the 2D image plane using the camera's perspective. This projection is crucial for the Elliptical Weighted Average (EWA) splatting technique, which helps in rendering high-quality images by reducing aliasing and improving image smoothness.

- **Projection to 2D:** Using the camera's view matrix and projection matrix, the 3D covariance matrix is transformed into the 2D image plane. This involves adjusting the covariance matrix with the cameraâ€™s field of view and focal length to accurately represent the spread of each Gaussian in image coordinates.
- **EWA Splatting:** Once in 2D, the covariance matrices are used in EWA splatting. This technique weights the contribution of each Gaussian kernel to the final image based on its elliptical footprint in the image plane, oriented and scaled according to the 2D covariance matrix. It effectively allows for the blending of contributions from multiple points, significantly enhancing the image quality by smoothing out noise and reducing sampling artifacts.

```python
def build_covariance_2d(
    mean3d, cov3d, viewmatrix, fov_x, fov_y, focal_x, focal_y
):
    # The following models the steps outlined by equations 29
	# and 31 in "EWA Splatting" (Zwicker et al., 2002). 
	# Additionally considers aspect / scaling of viewport.
	# Transposes used to account for row-/column-major conventions.
    tan_fovx = math.tan(fov_x * 0.5)
    tan_fovy = math.tan(fov_y * 0.5)
    t = (mean3d @ viewmatrix[:3,:3]) + viewmatrix[-1:,:3]

    # truncate the influences of gaussians far outside the frustum.
    tx = (t[..., 0] / t[..., 2]).clip(min=-tan_fovx*1.3, max=tan_fovx*1.3) * t[..., 2]
    ty = (t[..., 1] / t[..., 2]).clip(min=-tan_fovy*1.3, max=tan_fovy*1.3) * t[..., 2]
    tz = t[..., 2]

    # Eq.29 locally affine transform 
    # perspective transform is not affine so we approximate with first-order taylor expansion
    # notice that we multiply by the intrinsic so that the variance is at the sceen space
    J = torch.zeros(mean3d.shape[0], 3, 3).to(mean3d)
    J[..., 0, 0] = 1 / tz * focal_x
    J[..., 0, 2] = -tx / (tz * tz) * focal_x
    J[..., 1, 1] = 1 / tz * focal_y
    J[..., 1, 2] = -ty / (tz * tz) * focal_y
    # J[..., 2, 0] = tx / t.norm(dim=-1) # discard
    # J[..., 2, 1] = ty / t.norm(dim=-1) # discard
    # J[..., 2, 2] = tz / t.norm(dim=-1) # discard
    W = viewmatrix[:3,:3].T # transpose to correct viewmatrix
    cov2d = J @ W @ cov3d @ W.T @ J.permute(0,2,1)
    
    # add low pass filter here according to E.q. 32
    filter = torch.eye(2,2).to(cov2d) * 0.3
    return cov2d[:, :2, :2] + filter[None]
```

Understanding and accurately calculating 2D and 3D covariances are pivotal in rendering workflows that use Gaussian models and EWA splatting. These calculations allow for precise control over how data is visualized, directly impacting the fidelity and realism of the rendered images. This step is not just about transforming data from 3D to 2D but about preserving and enhancing visual information through sophisticated mathematical transformations and rendering techniques.

### The Rendering Method: Bringing Gaussian Models to Life

Once the preparation of 2D covariances and other necessary transformations is complete, the next step in our pipeline involves the actual rendering of these prepared models onto the 2D image plane. This process is facilitated by a sophisticated rendering method that efficiently integrates Gaussian splatting with modern graphics processing techniques.

#### Overview of the Rendering Method

The rendering method in our Gaussian Renderer is designed to take full advantage of the properties of Gaussian functions, specifically their ability to merge smoothly when overlapped. This capability is crucial for achieving high-quality, anti-aliased images in computational rendering.

#### Key Steps in the Rendering Process

1. **Projection to Normalized Device Coordinates (NDC):**
   - Each 3D point, along with its calculated 2D covariance, is projected into the NDC space using the camera's projection matrix. This step transforms 3D world coordinates into a standardized 2D coordinate system that simplifies further processing.

2. **Color and Opacity Calculation:**
   - For each point, color and opacity values are derived from the point's attributes, which may include direct color information or indices to a color map. Opacity plays a crucial role in determining how much a particular point will contribute to the final image, especially in regions where multiple points overlap.

3. **EWA Splatting on 2D Plane:**
   - The Elliptical Weighted Average (EWA) splatting technique is used to render each Gaussian onto the 2D plane. By using the 2D covariances, the method calculates how each point's Gaussian footprint affects its surrounding pixels, effectively "splatting" the Gaussian onto the image based on its shape and orientation.
   - This step involves computing the influence of each Gaussian on its neighboring pixels, weighted by the Gaussian's amplitude and its distance from the pixel center, adjusted for the Gaussian's elliptical shape.

4. **Depth and Visibility Considerations:**
   - Depth buffering is employed to ensure that only the nearest (visible) surface is rendered when multiple points project onto the same pixel. This is critical for correctly rendering overlapping surfaces where closer objects need to obscure farther ones.
   - A visibility filter based on the Gaussian's radii is used to determine which parts of the Gaussian contribute to the image, enhancing performance by avoiding unnecessary calculations for parts of the Gaussian that have negligible impact.

```python
def render(self, camera, means2D, cov2d, color, opacity, depths):
        radii = get_radius(cov2d)
        rect = get_rect(means2D, radii, width=camera.image_width, height=camera.image_height)
        
        self.render_color = torch.ones(*self.pix_coord.shape[:2], 3).to('cuda')
        self.render_depth = torch.zeros(*self.pix_coord.shape[:2], 1).to('cuda')
        self.render_alpha = torch.zeros(*self.pix_coord.shape[:2], 1).to('cuda')

        TILE_SIZE = 16
        for h in range(0, camera.image_height, TILE_SIZE):
            for w in range(0, camera.image_width, TILE_SIZE):
                # check if the rectangle penetrate the tile
                over_tl = rect[0][..., 0].clip(min=w), rect[0][..., 1].clip(min=h)
                over_br = rect[1][..., 0].clip(max=w+TILE_SIZE-1), rect[1][..., 1].clip(max=h+TILE_SIZE-1)
                in_mask = (over_br[0] > over_tl[0]) & (over_br[1] > over_tl[1]) # 3D gaussian in the tile 
                
                if not in_mask.sum() > 0:
                    continue

                P = in_mask.sum()
                tile_coord = self.pix_coord[h:h+TILE_SIZE, w:w+TILE_SIZE].flatten(0,-2)
                sorted_depths, index = torch.sort(depths[in_mask])
                sorted_means2D = means2D[in_mask][index]
                sorted_cov2d = cov2d[in_mask][index] # P 2 2
                sorted_conic = sorted_cov2d.inverse() # inverse of variance
                sorted_opacity = opacity[in_mask][index]
                sorted_color = color[in_mask][index]
                dx = (tile_coord[:,None,:] - sorted_means2D[None,:]) # B P 2
                
                gauss_weight = torch.exp(-0.5 * (
                    dx[:, :, 0]**2 * sorted_conic[:, 0, 0] 
                    + dx[:, :, 1]**2 * sorted_conic[:, 1, 1]
                    + dx[:,:,0]*dx[:,:,1] * sorted_conic[:, 0, 1]
                    + dx[:,:,0]*dx[:,:,1] * sorted_conic[:, 1, 0]))
                
                alpha = (gauss_weight[..., None] * sorted_opacity[None]).clip(max=0.99) # B P 1
                T = torch.cat([torch.ones_like(alpha[:,:1]), 1-alpha[:,:-1]], dim=1).cumprod(dim=1)
                acc_alpha = (alpha * T).sum(dim=1)
                tile_color = (T * alpha * sorted_color[None]).sum(dim=1) + (1-acc_alpha) * (1 if self.white_bkgd else 0)
                tile_depth = ((T * alpha) * sorted_depths[None,:,None]).sum(dim=1)
                self.render_color[h:h+TILE_SIZE, w:w+TILE_SIZE] = tile_color.reshape(TILE_SIZE, TILE_SIZE, -1)
                self.render_depth[h:h+TILE_SIZE, w:w+TILE_SIZE] = tile_depth.reshape(TILE_SIZE, TILE_SIZE, -1)
                self.render_alpha[h:h+TILE_SIZE, w:w+TILE_SIZE] = acc_alpha.reshape(TILE_SIZE, TILE_SIZE, -1)
```

#### Final Compositing:

After all points have been splatted onto the 2D plane, the final image is composed by aggregating all contributions. This step involves summing up the influences of all visible Gaussians on each pixel, taking into account their respective colors and opacities. The result is a smooth, anti-aliased image that represents the combined effect of all rendered points.

The rendering method employed by the Gaussian Renderer is a pivotal component of our visualization pipeline, transforming raw 3D data into compelling 2D images. By meticulously calculating the influence of each Gaussian on the image and carefully managing depth and visibility, this method ensures that the final visual output is both accurate and aesthetically pleasing.

The provided Python script is integral to understanding how spherical harmonics (SH) coefficients are used to calculate color information in applications such as gaussian splatting. Let's break down the key functions and their roles in this process:

## Spherical Harmonics
---
#### Functions Overview

- `RGB2SH(rgb)`: This function converts RGB color values into spherical harmonics coefficients. The transformation is performed by normalizing the RGB values (shifting by -0.5) and scaling by a constant \( C0 \).
  
- `SH2RGB(sh)`: This function converts spherical harmonics coefficients back into RGB color values. It essentially reverses the operation of `RGB2SH`, scaling by \( C0 \) and then shifting by +0.5 to retrieve the original RGB values.

#### Constants and Coefficients
The script defines several constants, \( C0 \), \( C1 \), \( C2 \), \( C3 \), and \( C4 \), which are used to scale the SH coefficients during the color calculations. These constants are derived from the properties of the spherical harmonics functions and their normalization in computer graphics.

#### Spherical Harmonics Evaluation (`eval_sh`)
This is the central function where the spherical harmonics are evaluated given specific directions. The function takes three arguments:
- `deg`: The degree of spherical harmonics. Currently, degrees 0 through 3 are supported.
- `sh`: An array of spherical harmonics coefficients.
- `dirs`: Unit directions for which the SH values need to be computed.

The function works by first multiplying the zeroth SH coefficient with \( C0 \). If higher degrees are involved, it incrementally calculates the influence of each additional degree using the provided unit directions and corresponding SH coefficients. The complexity and the number of terms increase with the degree of the harmonics, involving products of the direction components and the corresponding coefficients.

For instance, for the first degree:
- It uses the x, y, z components of the direction to modify the result based on \( C1 \) scaled SH coefficients.

For the second and higher degrees:
- Products of direction components (like {{< mathjax/inline>}}\( x \cdot y \){{< /mathjax/inline>}}, {{< mathjax/inline>}}\( x^2 \){{< /mathjax/inline>}}, etc.) are used alongside the {{< mathjax/inline>}}\( C2 \){{< /mathjax/inline>}}, {{< mathjax/inline>}}\( C3 \){{< /mathjax/inline>}}, and {{< mathjax/inline>}}\( C4 \){{< /mathjax/inline>}} coefficients to account for the increased complexity of higher degree harmonics.

This detailed handling of SH coefficients and directional components allows for accurate representation and manipulation of lighting and color in graphics, crucial for realistic rendering in applications like Gaussian splatting. For a detailed explanation of Spherical Harmonics take a look at [{{< mathjax/inline>}}<span style="color: #ffa700;">Overview of Spherical Harmonics in Gaussian Splatting</span>{{< /mathjax/inline>}}]({{< ref "blogs/spherical-harmonics" >}})

## Conclusion
---
In this post, we've explored the intricate processes behind transforming synthetic datasets into rich point clouds, the implementation of advanced camera techniques for Gaussian Rendering using EWA Splatting, and the sophisticated methods of rendering Gaussians. Additionally, we delved into the utilization of spherical harmonics for calculating the color dynamics in Gaussian rendering, underscoring the blend of mathematical rigor and computational efficiency that defines our approach. These techniques not only push the boundaries of what's achievable in computer graphics and 3D visualization but also offer scalable solutions for industry professionals and enthusiasts alike. By continuously refining these methods, we aim to bridge the gap between theoretical computer graphics and practical, stunning visual applications, ensuring that the future of rendering is both bright and incredibly detailed.